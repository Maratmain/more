# LLM Local Service Configuration
# llama.cpp server configuration

# Engine and Model Configuration
LLM_ENGINE=llama.cpp
LLM_MODEL=Qwen2.5-7B-Instruct-Q5_K_M.gguf
LLM_MODEL_URL=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q5_k_m.gguf

# Model Parameters
LLM_CTX=8192
LLM_THREADS=8
LLM_PORT=8080

# JSON Schema Enforcement
LLM_JSON_SCHEMA_ENFORCE=true

# GPU Configuration (auto-detect if available)
LLM_GPU_LAYERS=auto
LLM_GPU_MEMORY_UTILIZATION=0.9

# Server Configuration
LLM_HOST=0.0.0.0
LLM_PARALLEL=4
LLM_EMBEDDING=true
LLM_CHAT_TEMPLATE=qwen2

# Performance Tuning
LLM_BATCH_SIZE=512
LLM_UBATCH_SIZE=512
LLM_SEQ_MAX=2048
LLM_N_PREDICT=512

# Logging and Debug
LLM_VERBOSE=false
LLM_LOG_LEVEL=info

# Model Download Configuration
LLM_DOWNLOAD_TIMEOUT=3600
LLM_DOWNLOAD_RETRIES=3
LLM_MODEL_CHECKSUM=sha256:auto

# Alternative Models (commented out)
# LLM_MODEL=Qwen2.5-7B-Instruct-Q4_K_M.gguf
# LLM_MODEL_URL=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf

# LLM_MODEL=Qwen2.5-7B-Instruct-Q8_0.gguf
# LLM_MODEL_URL=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q8_0.gguf

# LLM_MODEL=Qwen2.5-7B-Instruct-F16.gguf
# LLM_MODEL_URL=https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-f16.gguf
